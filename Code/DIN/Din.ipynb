{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Din.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in2932avT76I",
        "outputId": "326590d6-e9d4-40e9-e187-180bab9bec2a"
      },
      "source": [
        "! pip install deepctr"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepctr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/23/a0c89b3a1631f8017dde94ee096db6ba14dfe0c996df8d5a0bdfb795ca54/deepctr-0.8.5-py3-none-any.whl (116kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from deepctr) (2.23.0)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deepctr) (2.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr) (2.10)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deepctr) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deepctr) (1.15.0)\n",
            "Installing collected packages: deepctr\n",
            "Successfully installed deepctr-0.8.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oghuYdEIT2OE"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import namedtuple\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
        "from deepctr.feature_column import SparseFeat, DenseFeat, VarLenSparseFeat"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNR5BI8QT0i1",
        "outputId": "31e0ceb1-77e9-44ab-b9e5-2ef7afbb2f69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "kS8gDAl7U3Dj",
        "outputId": "caff83df-ae13-4dea-ff3a-14d2d28e7a95"
      },
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/dl_group/modtrain.csv', sep=\"\\t\")\n",
        "data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>size</th>\n",
              "      <th>model_attr</th>\n",
              "      <th>user_attr</th>\n",
              "      <th>user_id</th>\n",
              "      <th>brand</th>\n",
              "      <th>category</th>\n",
              "      <th>rating</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>hist_movie_id</th>\n",
              "      <th>hist_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0 0 0 0 0 0 0 0 0 0 0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>0 65 0 0 0 0 0 0 0 0 0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0 65 1 0 0 0 0 0 0 0 0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71</td>\n",
              "      <td>0 65 1 5 0 0 0 0 0 0 0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0 65 1 5 13 0 0 0 0 0 0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68593</th>\n",
              "      <td>68593</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7621</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>43 12 48 52 23 0 0 0 0 0 0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68594</th>\n",
              "      <td>68594</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7621</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>43 12 48 52 23 87 0 0 0 0 0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68595</th>\n",
              "      <td>68595</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7621</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>43 12 48 52 23 87 55 0 0 0 0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68596</th>\n",
              "      <td>68596</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7621</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>43 12 48 52 23 87 55 24 0 0 0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68597</th>\n",
              "      <td>68597</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7621</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>43 12 48 52 23 87 55 24 88 0 0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68598 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  size  ...                   hist_movie_id  hist_len\n",
              "0               0   4.0  ...           0 0 0 0 0 0 0 0 0 0 0         1\n",
              "1               1   4.0  ...          0 65 0 0 0 0 0 0 0 0 0         2\n",
              "2               2   4.0  ...          0 65 1 0 0 0 0 0 0 0 0         3\n",
              "3               3   4.0  ...          0 65 1 5 0 0 0 0 0 0 0         4\n",
              "4               4   4.0  ...         0 65 1 5 13 0 0 0 0 0 0         5\n",
              "...           ...   ...  ...                             ...       ...\n",
              "68593       68593   4.0  ...      43 12 48 52 23 0 0 0 0 0 0         5\n",
              "68594       68594   4.0  ...     43 12 48 52 23 87 0 0 0 0 0         6\n",
              "68595       68595   4.0  ...    43 12 48 52 23 87 55 0 0 0 0         7\n",
              "68596       68596   4.0  ...   43 12 48 52 23 87 55 24 0 0 0         8\n",
              "68597       68597   4.0  ...  43 12 48 52 23 87 55 24 88 0 0         9\n",
              "\n",
              "[68598 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xluoj0o5lt4h"
      },
      "source": [
        "data=data[['user_attr', 'user_id', 'brand','category', 'rating', 'movie_id', 'hist_movie_id', 'hist_len']]\n",
        "data.columns = ['gender', 'user_id', 'age','movie_type_id', 'label', 'movie_id', 'hist_movie_id', 'hist_len']\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj_izfIknNrr"
      },
      "source": [
        "X_train = {\"user_id\": np.array(X[\"user_id\"]), \\\n",
        "            \"gender\": np.array(X[\"gender\"]), \\\n",
        "            \"age\": np.array(X[\"age\"]), \\\n",
        "            \"hist_movie_id\": np.array([[int(i) for i in l.split(' ')] for l in X[\"hist_movie_id\"]]), \\\n",
        "            \"hist_len\": np.array(X[\"hist_len\"]), \\\n",
        "            \"movie_id\": np.array(X[\"movie_id\"]), \\\n",
        "            \"movie_type_id\": np.array(X[\"movie_type_id\"])}\n",
        "y_train = np.array(y)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1DSO0j6nerW"
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "SparseFeat = namedtuple('SparseFeat', ['name', 'vocabulary_size', 'embedding_dim'])\n",
        "DenseFeat = namedtuple('DenseFeat', ['name', 'dimension'])\n",
        "VarLenSparseFeat = namedtuple('VarLenSparseFeat', ['name', 'vocabulary_size', 'embedding_dim', 'maxlen'])\n",
        "\n",
        "feature_columns = [SparseFeat('user_id', max(data[\"user_id\"])+1, embedding_dim=8),\n",
        "                    SparseFeat('gender', max(data[\"gender\"])+1, embedding_dim=8),\n",
        "                    SparseFeat('age', max(data[\"age\"])+1, embedding_dim=8),\n",
        "                    SparseFeat('movie_id', max(data[\"movie_id\"])+1, embedding_dim=8),\n",
        "                    SparseFeat('movie_type_id', max(data[\"movie_type_id\"])+1, embedding_dim=8),\n",
        "                    DenseFeat('hist_len', 1)]\n",
        "feature_columns += [VarLenSparseFeat('hist_movie_id', vocabulary_size=max(data[\"movie_id\"])+1, embedding_dim=8, maxlen=50)]\n",
        "\n",
        "\n",
        "behavior_feature_list = ['movie_id']\n",
        "behavior_seq_feature_list = ['hist_movie_id']\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3S2bIZNu1b3"
      },
      "source": [
        "# input\n",
        "def build_input_layers(feature_columns):\n",
        "    input_layer_dict = {}\n",
        "\n",
        "    for fc in feature_columns:\n",
        "        if isinstance(fc, SparseFeat):\n",
        "            input_layer_dict[fc.name] = Input(shape=(1,), name=fc.name)\n",
        "        elif isinstance(fc, DenseFeat):\n",
        "            input_layer_dict[fc.name] = Input(shape=(fc.dimension,), name=fc.name)\n",
        "        elif isinstance(fc, VarLenSparseFeat):\n",
        "            input_layer_dict[fc.name] = Input(shape=(fc.maxlen,), name=fc.name)\n",
        "\n",
        "    return input_layer_dict\n",
        "\n",
        "\n",
        "# embedding\n",
        "def build_embedding_layers(feature_columns, input_layer_dict):\n",
        "    embedding_layer_dict = {}\n",
        "\n",
        "    for fc in feature_columns:\n",
        "        if isinstance(fc, SparseFeat):\n",
        "            embedding_layer_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='emb_' + fc.name)\n",
        "        elif isinstance(fc, VarLenSparseFeat):\n",
        "            embedding_layer_dict[fc.name] = Embedding(fc.vocabulary_size + 1, fc.embedding_dim, name='emb_' + fc.name, mask_zero=True)\n",
        "\n",
        "    return embedding_layer_dict\n",
        "\n",
        "\n",
        "def embedding_lookup(feature_columns, input_layer_dict, embedding_layer_dict):\n",
        "    embedding_list = []\n",
        "\n",
        "    for fc in feature_columns:\n",
        "        _input = input_layer_dict[fc]\n",
        "        _embed = embedding_layer_dict[fc]\n",
        "        embed = _embed(_input)\n",
        "        embedding_list.append(embed)\n",
        "\n",
        "    return embedding_list\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLzl01-cv0Nh"
      },
      "source": [
        "# Dice\n",
        "class Dice(Layer):\n",
        "    def __init__(self):\n",
        "        super(Dice, self).__init__()\n",
        "        self.bn = BatchNormalization(center=False, scale=False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.alpha = self.add_weight(shape=(input_shape[-1],), dtype=tf.float32, name='alpha')\n",
        "\n",
        "    def call(self, x):\n",
        "        x_normed = self.bn(x)\n",
        "        x_p = tf.sigmoid(x_normed)\n",
        "\n",
        "        return self.alpha * (1.0 - x_p) * x + x_p * x"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI_I3BnYw3Cu"
      },
      "source": [
        "# ActivationUnit\n",
        "class LocalActivationUnit(Layer):\n",
        "    def __init__(self, hidden_units=(256, 128, 64), activation='prelu'):\n",
        "        super(LocalActivationUnit, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.linear = Dense(1)\n",
        "        self.dnn = [Dense(unit, activation=PReLU() if activation == 'prelu' else Dice()) for unit in self.hidden_units]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # query: item Embedding，keys: hist Embedding\n",
        "        query, keys = inputs\n",
        "        keys_len = keys.get_shape()[1]\n",
        "        queries = tf.tile(query, multiples=[1, keys_len, 1])\n",
        "        att_input = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1)\n",
        "        att_out = att_input\n",
        "        for fc in self.dnn:\n",
        "            att_out = fc(att_out)\n",
        "        att_out = self.linear(att_out)  # B x len x 1\n",
        "        att_out = tf.squeeze(att_out, -1)  # B x len\n",
        "\n",
        "        return att_out\n",
        "\n",
        "\n",
        "class AttentionPooling(Layer):\n",
        "    def __init__(self, att_hidden_units=(128, 64)):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.att_hidden_units = att_hidden_units\n",
        "        self.local_att = LocalActivationUnit(self.att_hidden_units, 'dice')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys = inputs\n",
        "        key_masks = tf.not_equal(keys[:, :, 0], 0)\n",
        "        attention_score = self.local_att([query, keys])\n",
        "        outputs = attention_score\n",
        "        paddings = tf.zeros_like(attention_score)\n",
        "        outputs = tf.where(key_masks, attention_score, paddings)\n",
        "        outputs = tf.expand_dims(outputs, axis=1)  # B x 1 x len\n",
        "        # keys : B x len x emb_dim\n",
        "        outputs = tf.matmul(outputs, keys)  # B x 1 x emb_dim\n",
        "        outputs = tf.squeeze(outputs, axis=1)  # B x emb_dim\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EgjSUX6gGOF"
      },
      "source": [
        "\n",
        "def get_dnn_logits(dnn_input, hidden_units=(200, 80), activation='prelu'):\n",
        "    dnns = [Dense(unit, activation=PReLU() if activation == 'prelu' else Dice()) for unit in hidden_units]\n",
        "\n",
        "    dnn_out = dnn_input\n",
        "    for dnn in dnns:\n",
        "        dnn_out = dnn(dnn_out)\n",
        "    dnn_logits = Dense(1, activation='sigmoid')(dnn_out)\n",
        "    return dnn_logits\n",
        "\n",
        "\n",
        "def concat_embedding_list(feature_columns, input_layer_dict, embedding_layer_dict, flatten=False):\n",
        "    embedding_list = []\n",
        "    for fc in feature_columns:\n",
        "        _input = input_layer_dict[fc.name]  \n",
        "        _embed = embedding_layer_dict[fc.name]  # B x 1 x dim  \n",
        "        embed = _embed(_input)  # B x dim  \n",
        "        if flatten:\n",
        "            embed = Flatten()(embed)\n",
        "\n",
        "        embedding_list.append(embed)\n",
        "\n",
        "    return embedding_list\n",
        "\n",
        "def concat_input_list(input_list):\n",
        "    feature_nums = len(input_list)\n",
        "    if feature_nums > 1:\n",
        "        return Concatenate(axis=1)(input_list)\n",
        "    elif feature_nums == 1:\n",
        "        return input_list[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list):\n",
        "    input_layer_dict = build_input_layers(feature_columns)\n",
        "    input_layers = list(input_layer_dict.values())\n",
        "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))\n",
        "    dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns))\n",
        "    dnn_dense_input = []\n",
        "    for fc in dense_feature_columns:\n",
        "        dnn_dense_input.append(input_layer_dict[fc.name])\n",
        "   \n",
        "    dnn_dense_input = concat_input_list(dnn_dense_input)\n",
        "    embedding_layer_dict = build_embedding_layers(feature_columns, input_layer_dict)\n",
        "\n",
        "    \n",
        "    dnn_sparse_emb_input = concat_embedding_list(sparse_feature_columns, input_layer_dict, embedding_layer_dict,\n",
        "                                                 flatten=True)\n",
        "    dnn_sparse_input = concat_input_list(dnn_sparse_emb_input)\n",
        "\n",
        "   \n",
        "    query_emb_list = embedding_lookup(behavior_feature_list, input_layer_dict, embedding_layer_dict)\n",
        "    keys_emb_list = embedding_lookup(behavior_seq_feature_list, input_layer_dict, embedding_layer_dict)\n",
        "\n",
        "    dnn_seq_input_list = []\n",
        "    for i in range(len(keys_emb_list)):\n",
        "        seq_emb = AttentionPooling()([query_emb_list[i], keys_emb_list[i]])\n",
        "        dnn_seq_input_list.append(seq_emb)\n",
        "    dnn_seq_input = concat_input_list(dnn_seq_input_list)\n",
        "\n",
        "    \n",
        "    dnn_input = Concatenate(axis=1)([dnn_dense_input, dnn_sparse_input, dnn_seq_input])\n",
        "    dnn_logits = get_dnn_logits(dnn_input, activation='dice')\n",
        "    model = Model(input_layers, dnn_logits)\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wYbEQe57v5Z"
      },
      "source": [
        "model = DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list)\n",
        "model.summary()\n",
        "learning_rate = 1e-4\n",
        "adam = tf.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(optimizer=adam,\n",
        "              loss=\"mse\",\n",
        "              metrics=[tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error'), tf.keras.metrics.MeanSquaredError(name='mean_squared_error')])\n",
        "history = model.fit(X_train, y_train, batch_size=12, epochs=5, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lGUj7fcJnO9"
      },
      "source": [
        "# plot\n",
        "def training_vis(hist):\n",
        "    loss = hist.history['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    mse = hist.history['mean_squared_error']\n",
        "    val_mse = hist.history['val_mean_squared_error']\n",
        "\n",
        "    rme = hist.history['root_mean_squared_error']\n",
        "    val_rme = hist.history['val_root_mean_squared_error']\n",
        "\n",
        "    # make a figure\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "    # subplot loss\n",
        "    ax1 = fig.add_subplot(131)\n",
        "    ax1.plot(loss, label='train_loss')\n",
        "    ax1.plot(val_loss, label='val_loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Loss on Training and Validation Data')\n",
        "    ax1.legend()\n",
        "    # subplot auc\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    ax2.plot(mse, label='train_mse')\n",
        "    ax2.plot(val_mse, label='val_mse')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('MSE')\n",
        "    ax2.set_title('MSE  on Training and Validation Data')\n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # subplot binary_crossentropy\n",
        "    ax3 = fig.add_subplot(133)\n",
        "    ax3.plot(rme, label='train_root_mean_squared_error')\n",
        "    ax3.plot(val_rme, label='val_root_mean_squared_error')\n",
        "    ax3.set_xlabel('Epochs')\n",
        "    ax3.set_ylabel('root_mean_squared_error')\n",
        "    ax3.set_title('RMSE on Training and Validation Data')\n",
        "    ax3.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "training_vis(history)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}